{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling\n",
    "\n",
    "You may have noticed that word2vec is really slow to train. Especially with big (> 50 000) vocabularies. Negative sampling is the solution.\n",
    "\n",
    "The task is to implement word2vec with negative sampling. In more detail: you should implement two ways of negative sampling.\n",
    "\n",
    "## Vanilla negative sampling\n",
    "\n",
    "This is what was discussed in Stanford lecture. The main idea is in the formula:\n",
    "\n",
    "$$ L = \\log\\sigma(u^T_o u_c) + \\sum^k_{i=1} \\mathbb{E}_{j \\sim P(w)}[\\log\\sigma(-u^T_j, u_c)]$$\n",
    "\n",
    "Where $\\sigma$ - sigmoid function, $u_c$ - central word vector, $u_o$ - context (outside of the window) word vector, $u_j$ - vector or word with index $j$.\n",
    "\n",
    "The first term calculates the similarity between positive examples (word from one window)\n",
    "\n",
    "The second term is responsible for negative samples. $k$ is a hyperparameter - the number of negatives to sample.\n",
    "$\\mathbb{E}_{j \\sim P(w)}$\n",
    "means that $j$ is distributed accordingly to unigram distribution, but it is better to use $P^{3/4}(w)$ (empirical results) and you can experiment with some other approaches (for example, try to use uniform distribution).\n",
    "\n",
    "Thus, it is only required to calculate the similarity between positive samples and some other negatives. Not across all the vocabulary.\n",
    "\n",
    "Useful links:\n",
    "1. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "1. [Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "## Batch-transpose trick for negative sampling\n",
    "\n",
    "But we can do better. Maybe we don't need to compute vectors for negative samples at all, because we already have a batch of training data and (hopefully) examples in the batch are highly decorrelated.\n",
    "\n",
    "Let's assume we work with Skip-gram model.\n",
    "\n",
    "Let $S$ be a batch of _L2-normalized_ word vectors `(batch_size, 2 * window_size + 1, word_vector_dim)`.\n",
    "\n",
    "```python\n",
    "x = 0.0\n",
    "for batch_idx in range(batch):\n",
    "    w = S[batch_idx, :, :]\n",
    "    x += np.sum(w.T @ w - 1.)\n",
    "\n",
    "y = 0.0\n",
    "for window_idx in range(window):\n",
    "    b = S[:, window_idx, :]\n",
    "    y += np.sum(b.T @ b)\n",
    "\n",
    "loss = -x + y```\n",
    "\n",
    "Think about this loss and compare it to vanilla negative sampling.\n",
    "\n",
    "Implement word2vec with batch-transpose trick. Modify the formula, if needed.\n",
    "\n",
    "If you are interested: [more info](https://www.tensorflow.org/extras/candidate_sampling.pdf) on other methods of candidate sampling.\n",
    "\n",
    "**Results of this task** are the very same as in task 3, **plus**:\n",
    " * implement two models (one with vanilla negative sampling and the other with batch-transpose trick)\n",
    " * compare all of the models from tasks 3-5. In terms of time and number of iterations until convergence and the quality of the resulting vectors.\n",
    " * answer the questions\n",
    "\n",
    "### Questions:\n",
    "1. Explain the batch-transpose trick formula in your own words. How would you name x, y, w and b?\n",
    "1. Should it be modified to serve as a word2vec loss? If yes, how?\n",
    "1. Is it possible to do the same trick with CBOW model? If yes, how?\n",
    "1. Does it matter how the batch is made in the case of batch-transpose trick? In the case of vanilla negative sampling?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "  1. w is \n",
    "  1. _\n",
    "  1. _\n",
    "  1. _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skipgram import NegativeSamplingBatcher, NegativeSamplingSkipGram\n",
    "import torch\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for visualisation\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select whether to train model during this run or just load it from saved file\n",
    "TRAIN = False\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 1024\n",
    "EMBEDDINGS_DIM = 100\n",
    "EPOCH_NUM = 2\n",
    "WINDOW_SIZE = 3\n",
    "N_NEGATIVE_SAMPLES = 15\n",
    "LOGS_PERIOD = 50\n",
    "MODEL_NAME = 'neg_sam'\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "using <class 'torch.LongTensor'> type\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "if device.__str__() == 'cpu':\n",
    "    tensor_type = torch.LongTensor\n",
    "else:\n",
    "    tensor_type = torch.cuda.LongTensor\n",
    "\n",
    "print('using device:', device)\n",
    "print(f'using {tensor_type} type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus into batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "with open('./data/text8', 'r') as text8:\n",
    "    text = text8.read().split()\n",
    "\n",
    "# text = ['first', 'used', 'against', 'early', 'working', 'radicals', 'including', 'class', 'other']\n",
    "batcher = NegativeSamplingBatcher(text, vocab_size=VOCAB_SIZE, window_size=WINDOW_SIZE,\n",
    "                                  batch_size=BATCH_SIZE, n_negative_examples=N_NEGATIVE_SAMPLES)\n",
    "# free memory\n",
    "text = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    loss_history = []\n",
    "    corpus_size = len(batcher._corpus_tokens)\n",
    "\n",
    "    model = NegativeSamplingSkipGram(VOCAB_SIZE, EMBEDDINGS_DIM)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learning_started = datetime.datetime.now()\n",
    "\n",
    "    cumulative_loss = 0\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for i, (context, target, negatives) in enumerate(batcher):\n",
    "            if len(context) < BATCH_SIZE:\n",
    "                continue\n",
    "            # Transform tokens from numpy to torch.Tensor\n",
    "            tensor_context = torch.from_numpy(context).type(tensor_type)\n",
    "            tensor_target = torch.from_numpy(target).type(tensor_type)\n",
    "            tensor_negatives = torch.from_numpy(negatives).type(tensor_type)\n",
    "            # Send tensors to the selected device\n",
    "            tensor_context.to(device)\n",
    "            tensor_target.to(device)\n",
    "            tensor_negatives.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss = model(tensor_target, tensor_context, tensor_negatives)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss\n",
    "\n",
    "            if i % LOGS_PERIOD == 0:\n",
    "                print(f'loss on {(i * BATCH_SIZE / corpus_size) * 100:.1f}%:' + \\\n",
    "                      f'{(cumulative_loss / LOGS_PERIOD) :.7f}')\n",
    "                loss_history.append(loss.data.cpu().item())\n",
    "                cumulative_loss = 0\n",
    "        \n",
    "        \n",
    "        # after every epoch we save:\n",
    "                                    # the model\n",
    "                                    # loss history\n",
    "        learning_ended = datetime.datetime.now()\n",
    "        learning_time = (learning_ended - learning_started).total_seconds()\n",
    "        learning_ended = learning_ended.strftime(\"%H-%M %d-%m-%Y\")\n",
    "\n",
    "        torch.save(model, f'./models/{MODEL_NAME}(finished-{learning_ended})(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                   f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                   f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                   f'.pytorchmodel')\n",
    "\n",
    "        with open(f'./data/loss/loss_{MODEL_NAME}(finished-{learning_ended})(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                  f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                  f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                  f'.pickle', 'wb') as f:\n",
    "            pickle.dump(loss_history, f)\n",
    "        print('Model saved succesfully')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_moving_average\n",
    "\n",
    "if not TRAIN:\n",
    "    loss_file_location = './data/loss/neg_sam(finished-19-02 08-03-2019)(epochs_completed-1)(epoch_num-2)(vocab-20000)(batch-1024)(emb-100)(wind-3)(consumed-778.097815).pickle'\n",
    "    with open(loss_file_location, 'rb') as f:\n",
    "        loss_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(pd.Series(loss_history), 64, plot_actual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to map token (and word) to corresponding word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EmbeddingsEval\n",
    "if not TRAIN:\n",
    "    model_file_location ='./models/neg_sam(finished-19-02 08-03-2019)(epochs_completed-1)(epoch_num-2)(vocab-20000)(batch-1024)(emb-100)(wind-3)(consumed-778.097815).pytorchmodel'\n",
    "    model = torch.load(model_file_location, map_location='cpu')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_matrix = model.get_intrinsic_matrix()\n",
    "\n",
    "emb_eval = EmbeddingsEval(intrinsic_matrix, words_to_tokens=batcher.words_to_tokens,\n",
    "                          tokens_to_words=batcher.tokens_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eval.tokens_to_embeddings([1, 2, 3])\n",
    "emb_eval.words_to_embeddings(['integrity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beautiful visualizations\n",
    "Take top n most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 200\n",
    "words = batcher.tokens_to_words(np.arange(0, num_words))\n",
    "embeddings = emb_eval.tokens_to_embeddings(np.arange(0, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "points2d = pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.scatter(points2d[:, 0], points2d[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (points2d[i, 0] + 0.01, points2d[i, 1] + 0.01), fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qualitative evaluations of word vectors: nearest neighbors, word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_list in emb_eval.words_to_neighbors(['war', 'france', 'drink']):\n",
    "    print(batcher.tokens_to_words(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = emb_eval.most_similar(positive=['france', 'berlin'], negative=['paris'])\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
