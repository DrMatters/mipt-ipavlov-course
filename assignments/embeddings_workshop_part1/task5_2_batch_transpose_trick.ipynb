{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch-transpose trick for negative sampling\n",
    "\n",
    "But we can do better. Maybe we don't need to compute vectors for negative samples at all, because we already have a batch of training data and (hopefully) examples in the batch are highly decorrelated.\n",
    "\n",
    "Let's assume we work with Skip-gram model.\n",
    "\n",
    "Let $S$ be a batch of _L2-normalized_ word vectors `(batch_size, 2 * window_size + 1, word_vector_dim)`.\n",
    "\n",
    "```python\n",
    "x = 0.0\n",
    "for batch_idx in range(batch):\n",
    "    w = S[batch_idx, :, :]\n",
    "    x += np.sum(w.T @ w - 1.)\n",
    "\n",
    "y = 0.0\n",
    "for window_idx in range(window):\n",
    "    b = S[:, window_idx, :]\n",
    "    y += np.sum(b.T @ b)\n",
    "\n",
    "loss = -x + y```\n",
    "\n",
    "Think about this loss and compare it to vanilla negative sampling.\n",
    "\n",
    "Implement word2vec with batch-transpose trick. Modify the formula, if needed.\n",
    "\n",
    "If you are interested: [more info](https://www.tensorflow.org/extras/candidate_sampling.pdf) on other methods of candidate sampling.\n",
    "\n",
    "**Results of this task** are the very same as in task 3, **plus**:\n",
    " * implement two models (one with vanilla negative sampling and the other with batch-transpose trick)\n",
    " * compare all of the models from tasks 3-5. In terms of time and number of iterations until convergence and the quality of the resulting vectors.\n",
    " * answer the questions\n",
    "\n",
    "### Questions:\n",
    "1. Explain the batch-transpose trick formula in your own words. How would you name x, y, w and b?\n",
    "1. Should it be modified to serve as a word2vec loss? If yes, how?\n",
    "1. Is it possible to do the same trick with CBOW model? If yes, how?\n",
    "1. Does it matter how the batch is made in the case of batch-transpose trick? In the case of vanilla negative sampling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers:\n",
    "  1. (w @ w.T) - is something like a similarity matrix, that shows the similarity of w2vecs within one batch. x is the sum of this similarity matrix we want to maximize it. (b @ b.T) - is the opposite: it shows similarity between examples of batch. y is the sum of the second matrix and because we assume that they are highly decorrelated we want to minimize it.\n",
    "  1. Probably we should use np.mean, rather than np.sum to this loss serve as word2vec loss.\n",
    "  1. I think no, it's not possible. CBOW uses all information from 2 * window_size at the same moment, unlike Skip-Gram which uses information from all combination of the center word and context word within 2 * window_size.\n",
    "  1. In batch-transpose trick examples within batch should be highly decorrelated to work properly. In vanilla negative sampling positive pairs probably can be formed from corpus without shuffling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import skipgram\n",
    "import torch\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for visualisation\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select whether to train model during this run or just load it from saved file\n",
    "TRAIN = True\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 400\n",
    "EMBEDDINGS_DIM = 50\n",
    "EPOCH_NUM = 2\n",
    "WINDOW_SIZE = 3\n",
    "LOGS_PERIOD = 40\n",
    "MODEL_NAME = 'transpose_trick'\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    index_tensor_type = torch.cuda.LongTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    index_tensor_type = torch.LongTensor\n",
    "\n",
    "print('using device:', device)\n",
    "print(f'for index using {index_tensor_type} type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus into batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "with open('./data/text8', 'r') as text8:\n",
    "    text = text8.read().split()\n",
    "\n",
    "# text = ['first', 'used', 'against', 'early', 'working', 'class', 'radicals',\n",
    "#         'including', 'other', 'batch', 'look', 'going', 'down', 'inner', 'product']\n",
    "batcher = skipgram.TransposeTrickBatcher(text, vocab_size=VOCAB_SIZE, window_size=WINDOW_SIZE,\n",
    "                                         batch_size=BATCH_SIZE)\n",
    "# free memory\n",
    "text = []\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload module if it's changed on disk\n",
    "skipgram = importlib.reload(skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    loss_history = []\n",
    "    corpus_size = len(batcher._corpus_tokens)\n",
    "\n",
    "    model = skipgram.TransposeTrickSkipGram(VOCAB_SIZE, EMBEDDINGS_DIM)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learning_started = datetime.datetime.now()\n",
    "\n",
    "    cumulative_loss = 0\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for i, (batch) in enumerate(batcher):\n",
    "\n",
    "            # Transform tokens from numpy to torch.Tensor\n",
    "            tensor_batch = torch.from_numpy(batch).type(index_tensor_type)\n",
    "            # Send tensors to the selected device\n",
    "            tensor_batch.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss = model(tensor_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss.item()\n",
    "\n",
    "            if i % LOGS_PERIOD == 0:\n",
    "                print(f'loss on {(i * BATCH_SIZE / corpus_size) * 100:.1f}%:' + \\\n",
    "                      f'{(cumulative_loss / LOGS_PERIOD) :.7f}')\n",
    "                loss_history.append(cumulative_loss)\n",
    "                cumulative_loss = 0\n",
    "        \n",
    "        \n",
    "        # after every epoch we save:\n",
    "                                    # the model\n",
    "                                    # loss history\n",
    "        learning_ended = datetime.datetime.now()\n",
    "        learning_time = (learning_ended - learning_started).total_seconds()\n",
    "        learning_ended = learning_ended.strftime(\"%H-%M %d-%m-%Y\")\n",
    "\n",
    "        torch.save(model, f'./models/{MODEL_NAME}(finished-{learning_ended})(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                   f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                   f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                   f'.pytorchmodel')\n",
    "\n",
    "        with open(f'./data/loss/loss_{MODEL_NAME}(finished-{learning_ended})(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                  f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                  f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                  f'.pickle', 'wb') as f:\n",
    "            pickle.dump(loss_history, f)\n",
    "        print('Model saved succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_moving_average\n",
    "\n",
    "if not TRAIN:\n",
    "    loss_file_location = ''\n",
    "    with open(loss_file_location, 'rb') as f:\n",
    "        loss_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(pd.Series(loss_history), 64, plot_actual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to map token (and word) to corresponding word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EmbeddingsEval\n",
    "if not TRAIN:\n",
    "    model_file_location =''\n",
    "    model = torch.load(model_file_location, map_location='cpu')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_matrix = model.get_intrinsic_matrix()\n",
    "\n",
    "emb_eval = EmbeddingsEval(intrinsic_matrix, words_to_tokens=batcher.words_to_tokens,\n",
    "                          tokens_to_words=batcher.tokens_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eval.tokens_to_embeddings([1, 2, 3])\n",
    "emb_eval.words_to_embeddings(['integrity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beautiful visualizations\n",
    "Take top n most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 200\n",
    "words = batcher.tokens_to_words(np.arange(0, num_words))\n",
    "embeddings = emb_eval.tokens_to_embeddings(np.arange(0, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "points2d = pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.scatter(points2d[:, 0], points2d[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (points2d[i, 0] + 0.01, points2d[i, 1] + 0.01), fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qualitative evaluations of word vectors: nearest neighbors, word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_list in emb_eval.words_to_neighbors(['war', 'france', 'drink']):\n",
    "    print(batcher.tokens_to_words(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = emb_eval.most_similar(positive=['france', 'berlin'], negative=['paris'])\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
