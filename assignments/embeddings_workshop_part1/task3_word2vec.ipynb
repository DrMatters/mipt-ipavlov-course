{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skipgram import SkipGram, SkipGramBatcher\n",
    "import torch\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select whether to train model during this run (or just load it from saved file)\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 5000\n",
    "EMBEDDINGS_DIM = 150\n",
    "EPOCH_NUM = 2\n",
    "WINDOW_SIZE = 2\n",
    "LOGS_PERIOD = 10\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus into batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = []\n",
    "with open('./data/text8', 'r') as text8:\n",
    "    text = text8.read().split()\n",
    "\n",
    "# text = ['first', 'used', 'against', 'early', 'working', 'radicals', 'including', 'class', 'other']\n",
    "batcher = SkipGramBatcher(corpus=text, vocab_size=VOCAB_SIZE,\n",
    "                          batch_size=BATCH_SIZE, window_size=WINDOW_SIZE,\n",
    "                          drop_stop_words=True)\n",
    "# free memory\n",
    "text = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    loss_history = []\n",
    "    corpus_size = len(batcher.corpus_indexes)\n",
    "\n",
    "    model = SkipGram(VOCAB_SIZE, EMBEDDINGS_DIM)\n",
    "    model.to(device)\n",
    "    loss_fun = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative loss on 0.0%:0.0017351\n",
      "Cumulative loss on 0.5%:0.0171895\n",
      "Cumulative loss on 0.9%:0.0170728\n",
      "Cumulative loss on 1.4%:0.0170380\n",
      "Cumulative loss on 1.8%:0.0169965\n",
      "Cumulative loss on 2.3%:0.0169600\n",
      "Cumulative loss on 2.8%:0.0169549\n",
      "Cumulative loss on 3.2%:0.0169336\n",
      "Cumulative loss on 3.7%:0.0169338\n",
      "Cumulative loss on 4.1%:0.0169180\n",
      "Cumulative loss on 4.6%:0.0169036\n",
      "Cumulative loss on 5.1%:0.0168893\n",
      "Cumulative loss on 5.5%:0.0168745\n",
      "Cumulative loss on 6.0%:0.0168678\n",
      "Cumulative loss on 6.4%:0.0168598\n",
      "Cumulative loss on 6.9%:0.0168733\n",
      "Cumulative loss on 7.3%:0.0168465\n",
      "Cumulative loss on 7.8%:0.0168389\n",
      "Cumulative loss on 8.3%:0.0168537\n",
      "Cumulative loss on 8.7%:0.0168273\n",
      "Cumulative loss on 9.2%:0.0168165\n",
      "Cumulative loss on 9.6%:0.0168305\n",
      "Cumulative loss on 10.1%:0.0168073\n",
      "Cumulative loss on 10.6%:0.0168074\n",
      "Cumulative loss on 11.0%:0.0167964\n",
      "Cumulative loss on 11.5%:0.0168042\n",
      "Cumulative loss on 11.9%:0.0167806\n",
      "Cumulative loss on 12.4%:0.0167742\n",
      "Cumulative loss on 12.9%:0.0168031\n",
      "Cumulative loss on 13.3%:0.0167802\n",
      "Cumulative loss on 13.8%:0.0167635\n",
      "Cumulative loss on 14.2%:0.0167600\n",
      "Cumulative loss on 14.7%:0.0167567\n",
      "Cumulative loss on 15.2%:0.0167418\n",
      "Cumulative loss on 15.6%:0.0167604\n",
      "Cumulative loss on 16.1%:0.0167446\n",
      "Cumulative loss on 16.5%:0.0167466\n",
      "Cumulative loss on 17.0%:0.0167474\n",
      "Cumulative loss on 17.4%:0.0167311\n",
      "Cumulative loss on 17.9%:0.0167328\n",
      "Cumulative loss on 18.4%:0.0167141\n",
      "Cumulative loss on 18.8%:0.0167248\n",
      "Cumulative loss on 19.3%:0.0167059\n",
      "Cumulative loss on 19.7%:0.0166854\n",
      "Cumulative loss on 20.2%:0.0167021\n",
      "Cumulative loss on 20.7%:0.0166628\n",
      "Cumulative loss on 21.1%:0.0166905\n",
      "Cumulative loss on 21.6%:0.0166792\n",
      "Cumulative loss on 22.0%:0.0166844\n",
      "Cumulative loss on 22.5%:0.0166681\n",
      "Cumulative loss on 23.0%:0.0166760\n",
      "Cumulative loss on 23.4%:0.0166556\n",
      "Cumulative loss on 23.9%:0.0166502\n",
      "Cumulative loss on 24.3%:0.0166422\n",
      "Cumulative loss on 24.8%:0.0166449\n",
      "Cumulative loss on 25.3%:0.0166362\n",
      "Cumulative loss on 25.7%:0.0166384\n",
      "Cumulative loss on 26.2%:0.0166485\n",
      "Cumulative loss on 26.6%:0.0166252\n",
      "Cumulative loss on 27.1%:0.0166282\n",
      "Cumulative loss on 27.5%:0.0166252\n",
      "Cumulative loss on 28.0%:0.0166097\n",
      "Cumulative loss on 28.5%:0.0166262\n",
      "Cumulative loss on 28.9%:0.0166214\n",
      "Cumulative loss on 29.4%:0.0166040\n",
      "Cumulative loss on 29.8%:0.0165946\n",
      "Cumulative loss on 30.3%:0.0165909\n",
      "Cumulative loss on 30.8%:0.0165845\n",
      "Cumulative loss on 31.2%:0.0165864\n",
      "Cumulative loss on 31.7%:0.0165891\n",
      "Cumulative loss on 32.1%:0.0165692\n",
      "Cumulative loss on 32.6%:0.0165583\n",
      "Cumulative loss on 33.1%:0.0165557\n",
      "Cumulative loss on 33.5%:0.0165391\n",
      "Cumulative loss on 34.0%:0.0165451\n",
      "Cumulative loss on 34.4%:0.0165550\n",
      "Cumulative loss on 34.9%:0.0165461\n",
      "Cumulative loss on 35.4%:0.0165448\n",
      "Cumulative loss on 35.8%:0.0165388\n",
      "Cumulative loss on 36.3%:0.0165434\n",
      "Cumulative loss on 36.7%:0.0165353\n",
      "Cumulative loss on 37.2%:0.0165233\n",
      "Cumulative loss on 37.6%:0.0165218\n",
      "Cumulative loss on 38.1%:0.0165109\n",
      "Cumulative loss on 38.6%:0.0165209\n",
      "Cumulative loss on 39.0%:0.0165113\n",
      "Cumulative loss on 39.5%:0.0164955\n",
      "Cumulative loss on 39.9%:0.0164861\n",
      "Cumulative loss on 40.4%:0.0165071\n",
      "Cumulative loss on 40.9%:0.0164990\n",
      "Cumulative loss on 41.3%:0.0164915\n",
      "Cumulative loss on 41.8%:0.0164839\n",
      "Cumulative loss on 42.2%:0.0164820\n",
      "Cumulative loss on 42.7%:0.0164709\n",
      "Cumulative loss on 43.2%:0.0164737\n",
      "Cumulative loss on 43.6%:0.0164772\n",
      "Cumulative loss on 44.1%:0.0164813\n",
      "Cumulative loss on 44.5%:0.0164564\n",
      "Cumulative loss on 45.0%:0.0164569\n",
      "Cumulative loss on 45.5%:0.0164425\n",
      "Cumulative loss on 45.9%:0.0164285\n",
      "Cumulative loss on 46.4%:0.0164439\n",
      "Cumulative loss on 46.8%:0.0164608\n",
      "Cumulative loss on 47.3%:0.0164379\n",
      "Cumulative loss on 47.7%:0.0164361\n",
      "Cumulative loss on 48.2%:0.0164267\n",
      "Cumulative loss on 48.7%:0.0164405\n",
      "Cumulative loss on 49.1%:0.0164258\n",
      "Cumulative loss on 49.6%:0.0164300\n",
      "Cumulative loss on 50.0%:0.0164025\n",
      "Cumulative loss on 50.5%:0.0164265\n",
      "Cumulative loss on 51.0%:0.0164240\n",
      "Cumulative loss on 51.4%:0.0164078\n",
      "Cumulative loss on 51.9%:0.0163998\n",
      "Cumulative loss on 52.3%:0.0164014\n",
      "Cumulative loss on 52.8%:0.0163906\n",
      "Cumulative loss on 53.3%:0.0163921\n",
      "Cumulative loss on 53.7%:0.0163814\n",
      "Cumulative loss on 54.2%:0.0163800\n",
      "Cumulative loss on 54.6%:0.0164045\n",
      "Cumulative loss on 55.1%:0.0163868\n",
      "Cumulative loss on 55.6%:0.0163844\n",
      "Cumulative loss on 56.0%:0.0163874\n",
      "Cumulative loss on 56.5%:0.0163558\n",
      "Cumulative loss on 56.9%:0.0163756\n",
      "Cumulative loss on 57.4%:0.0163793\n",
      "Cumulative loss on 57.8%:0.0163733\n",
      "Cumulative loss on 58.3%:0.0163734\n",
      "Cumulative loss on 58.8%:0.0163619\n",
      "Cumulative loss on 59.2%:0.0163501\n",
      "Cumulative loss on 59.7%:0.0163525\n",
      "Cumulative loss on 60.1%:0.0163389\n",
      "Cumulative loss on 60.6%:0.0163486\n",
      "Cumulative loss on 61.1%:0.0163447\n",
      "Cumulative loss on 61.5%:0.0163442\n",
      "Cumulative loss on 62.0%:0.0163562\n",
      "Cumulative loss on 62.4%:0.0163469\n",
      "Cumulative loss on 62.9%:0.0163388\n",
      "Cumulative loss on 63.4%:0.0163305\n",
      "Cumulative loss on 63.8%:0.0163334\n",
      "Cumulative loss on 64.3%:0.0163502\n",
      "Cumulative loss on 64.7%:0.0163203\n",
      "Cumulative loss on 65.2%:0.0163313\n",
      "Cumulative loss on 65.7%:0.0163266\n",
      "Cumulative loss on 66.1%:0.0163170\n",
      "Cumulative loss on 66.6%:0.0163135\n",
      "Cumulative loss on 67.0%:0.0163169\n",
      "Cumulative loss on 67.5%:0.0163092\n",
      "Cumulative loss on 67.9%:0.0162975\n",
      "Cumulative loss on 68.4%:0.0163365\n",
      "Cumulative loss on 68.9%:0.0163032\n",
      "Cumulative loss on 69.3%:0.0163097\n",
      "Cumulative loss on 69.8%:0.0163100\n",
      "Cumulative loss on 70.2%:0.0162879\n",
      "Cumulative loss on 70.7%:0.0162748\n",
      "Cumulative loss on 71.2%:0.0162774\n",
      "Cumulative loss on 71.6%:0.0162963\n",
      "Cumulative loss on 72.1%:0.0162842\n",
      "Cumulative loss on 72.5%:0.0162916\n",
      "Cumulative loss on 73.0%:0.0162907\n",
      "Cumulative loss on 73.5%:0.0162589\n",
      "Cumulative loss on 73.9%:0.0162750\n",
      "Cumulative loss on 74.4%:0.0162840\n",
      "Cumulative loss on 74.8%:0.0162792\n",
      "Cumulative loss on 75.3%:0.0162722\n",
      "Cumulative loss on 75.8%:0.0162674\n",
      "Cumulative loss on 76.2%:0.0162830\n",
      "Cumulative loss on 76.7%:0.0162865\n",
      "Cumulative loss on 77.1%:0.0162839\n",
      "Cumulative loss on 77.6%:0.0162715\n",
      "Cumulative loss on 78.0%:0.0162501\n",
      "Cumulative loss on 78.5%:0.0162565\n",
      "Cumulative loss on 79.0%:0.0162531\n",
      "Cumulative loss on 79.4%:0.0162726\n",
      "Cumulative loss on 79.9%:0.0162497\n",
      "Cumulative loss on 80.3%:0.0162561\n",
      "Cumulative loss on 80.8%:0.0162488\n",
      "Cumulative loss on 81.3%:0.0162797\n",
      "Cumulative loss on 81.7%:0.0162457\n",
      "Cumulative loss on 82.2%:0.0162467\n",
      "Cumulative loss on 82.6%:0.0162576\n",
      "Cumulative loss on 83.1%:0.0162425\n",
      "Cumulative loss on 83.6%:0.0162289\n",
      "Cumulative loss on 84.0%:0.0162503\n",
      "Cumulative loss on 84.5%:0.0162333\n",
      "Cumulative loss on 84.9%:0.0162234\n",
      "Cumulative loss on 85.4%:0.0162466\n",
      "Cumulative loss on 85.9%:0.0162385\n",
      "Cumulative loss on 86.3%:0.0162287\n",
      "Cumulative loss on 86.8%:0.0162303\n",
      "Cumulative loss on 87.2%:0.0162377\n",
      "Cumulative loss on 87.7%:0.0162142\n",
      "Cumulative loss on 88.1%:0.0162134\n",
      "Cumulative loss on 88.6%:0.0162220\n",
      "Cumulative loss on 89.1%:0.0162230\n",
      "Cumulative loss on 89.5%:0.0162127\n",
      "Cumulative loss on 90.0%:0.0162216\n",
      "Cumulative loss on 90.4%:0.0161960\n",
      "Cumulative loss on 90.9%:0.0162139\n",
      "Cumulative loss on 91.4%:0.0161992\n",
      "Cumulative loss on 91.8%:0.0161995\n",
      "Cumulative loss on 92.3%:0.0162208\n",
      "Cumulative loss on 92.7%:0.0162128\n",
      "Cumulative loss on 93.2%:0.0162244\n",
      "Cumulative loss on 93.7%:0.0161999\n",
      "Cumulative loss on 94.1%:0.0161953\n",
      "Cumulative loss on 94.6%:0.0161858\n",
      "Cumulative loss on 95.0%:0.0162103\n",
      "Cumulative loss on 95.5%:0.0161985\n",
      "Cumulative loss on 96.0%:0.0162242\n",
      "Cumulative loss on 96.4%:0.0161984\n",
      "Cumulative loss on 96.9%:0.0161910\n",
      "Cumulative loss on 97.3%:0.0161857\n",
      "Cumulative loss on 97.8%:0.0161954\n",
      "Cumulative loss on 98.2%:0.0161977\n",
      "Cumulative loss on 98.7%:0.0161733\n",
      "Cumulative loss on 99.2%:0.0161841\n",
      "Cumulative loss on 99.6%:0.0161945\n",
      "Cumulative loss on 0.0%:0.0141472\n",
      "Cumulative loss on 0.5%:0.0160702\n",
      "Cumulative loss on 0.9%:0.0160842\n",
      "Cumulative loss on 1.4%:0.0160752\n",
      "Cumulative loss on 1.8%:0.0160821\n",
      "Cumulative loss on 2.3%:0.0160737\n",
      "Cumulative loss on 2.8%:0.0160813\n",
      "Cumulative loss on 3.2%:0.0160733\n",
      "Cumulative loss on 3.7%:0.0160705\n",
      "Cumulative loss on 4.1%:0.0161048\n",
      "Cumulative loss on 4.6%:0.0160803\n",
      "Cumulative loss on 5.1%:0.0160779\n",
      "Cumulative loss on 5.5%:0.0160908\n",
      "Cumulative loss on 6.0%:0.0160913\n",
      "Cumulative loss on 6.4%:0.0160728\n",
      "Cumulative loss on 6.9%:0.0160746\n",
      "Cumulative loss on 7.3%:0.0160775\n",
      "Cumulative loss on 7.8%:0.0160713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative loss on 8.3%:0.0160787\n",
      "Cumulative loss on 8.7%:0.0161089\n",
      "Cumulative loss on 9.2%:0.0160895\n",
      "Cumulative loss on 9.6%:0.0160778\n",
      "Cumulative loss on 10.1%:0.0160720\n",
      "Cumulative loss on 10.6%:0.0160886\n",
      "Cumulative loss on 11.0%:0.0160949\n",
      "Cumulative loss on 11.5%:0.0160940\n",
      "Cumulative loss on 11.9%:0.0160778\n",
      "Cumulative loss on 12.4%:0.0160817\n",
      "Cumulative loss on 12.9%:0.0161068\n",
      "Cumulative loss on 13.3%:0.0160893\n",
      "Cumulative loss on 13.8%:0.0160856\n",
      "Cumulative loss on 14.2%:0.0160657\n",
      "Cumulative loss on 14.7%:0.0161064\n",
      "Cumulative loss on 15.2%:0.0160872\n",
      "Cumulative loss on 15.6%:0.0160680\n",
      "Cumulative loss on 16.1%:0.0160889\n",
      "Cumulative loss on 16.5%:0.0160732\n",
      "Cumulative loss on 17.0%:0.0160944\n",
      "Cumulative loss on 17.4%:0.0160902\n",
      "Cumulative loss on 17.9%:0.0160795\n",
      "Cumulative loss on 18.4%:0.0160966\n",
      "Cumulative loss on 18.8%:0.0160789\n",
      "Cumulative loss on 19.3%:0.0160882\n",
      "Cumulative loss on 19.7%:0.0160910\n",
      "Cumulative loss on 20.2%:0.0160643\n",
      "Cumulative loss on 20.7%:0.0160721\n",
      "Cumulative loss on 21.1%:0.0160731\n",
      "Cumulative loss on 21.6%:0.0160976\n",
      "Cumulative loss on 22.0%:0.0160783\n",
      "Cumulative loss on 22.5%:0.0160591\n",
      "Cumulative loss on 23.0%:0.0160718\n",
      "Cumulative loss on 23.4%:0.0160659\n",
      "Cumulative loss on 23.9%:0.0160842\n",
      "Cumulative loss on 24.3%:0.0160953\n",
      "Cumulative loss on 24.8%:0.0160955\n",
      "Cumulative loss on 25.3%:0.0160885\n",
      "Cumulative loss on 25.7%:0.0160753\n",
      "Cumulative loss on 26.2%:0.0160763\n",
      "Cumulative loss on 26.6%:0.0160784\n",
      "Cumulative loss on 27.1%:0.0160742\n",
      "Cumulative loss on 27.5%:0.0160624\n",
      "Cumulative loss on 28.0%:0.0160709\n",
      "Cumulative loss on 28.5%:0.0160671\n",
      "Cumulative loss on 28.9%:0.0160825\n",
      "Cumulative loss on 29.4%:0.0160742\n",
      "Cumulative loss on 29.8%:0.0160672\n",
      "Cumulative loss on 30.3%:0.0160620\n",
      "Cumulative loss on 30.8%:0.0160612\n",
      "Cumulative loss on 31.2%:0.0160491\n",
      "Cumulative loss on 31.7%:0.0160694\n",
      "Cumulative loss on 32.1%:0.0160471\n",
      "Cumulative loss on 32.6%:0.0160641\n",
      "Cumulative loss on 33.1%:0.0160611\n",
      "Cumulative loss on 33.5%:0.0160776\n",
      "Cumulative loss on 34.0%:0.0160730\n",
      "Cumulative loss on 34.4%:0.0160575\n",
      "Cumulative loss on 34.9%:0.0160798\n",
      "Cumulative loss on 35.4%:0.0160804\n",
      "Cumulative loss on 35.8%:0.0160477\n",
      "Cumulative loss on 36.3%:0.0160545\n",
      "Cumulative loss on 36.7%:0.0160493\n",
      "Cumulative loss on 37.2%:0.0160490\n",
      "Cumulative loss on 37.6%:0.0160597\n",
      "Cumulative loss on 38.1%:0.0160566\n",
      "Cumulative loss on 38.6%:0.0160665\n",
      "Cumulative loss on 39.0%:0.0160412\n",
      "Cumulative loss on 39.5%:0.0160446\n",
      "Cumulative loss on 39.9%:0.0160343\n",
      "Cumulative loss on 40.4%:0.0160598\n",
      "Cumulative loss on 40.9%:0.0160553\n",
      "Cumulative loss on 41.3%:0.0160677\n",
      "Cumulative loss on 41.8%:0.0160588\n",
      "Cumulative loss on 42.2%:0.0160731\n",
      "Cumulative loss on 42.7%:0.0160284\n",
      "Cumulative loss on 43.2%:0.0160546\n",
      "Cumulative loss on 43.6%:0.0160428\n",
      "Cumulative loss on 44.1%:0.0160556\n",
      "Cumulative loss on 44.5%:0.0160462\n",
      "Cumulative loss on 45.0%:0.0160272\n",
      "Cumulative loss on 45.5%:0.0160517\n",
      "Cumulative loss on 45.9%:0.0160453\n",
      "Cumulative loss on 46.4%:0.0160396\n",
      "Cumulative loss on 46.8%:0.0160338\n",
      "Cumulative loss on 47.3%:0.0160495\n",
      "Cumulative loss on 47.7%:0.0160612\n",
      "Cumulative loss on 48.2%:0.0160186\n",
      "Cumulative loss on 48.7%:0.0160594\n",
      "Cumulative loss on 49.1%:0.0160599\n",
      "Cumulative loss on 49.6%:0.0160347\n",
      "Cumulative loss on 50.0%:0.0160635\n",
      "Cumulative loss on 50.5%:0.0160444\n",
      "Cumulative loss on 51.0%:0.0160421\n",
      "Cumulative loss on 51.4%:0.0160332\n",
      "Cumulative loss on 51.9%:0.0160263\n",
      "Cumulative loss on 52.3%:0.0160533\n",
      "Cumulative loss on 52.8%:0.0160307\n",
      "Cumulative loss on 53.3%:0.0160426\n",
      "Cumulative loss on 53.7%:0.0160542\n",
      "Cumulative loss on 54.2%:0.0160280\n",
      "Cumulative loss on 54.6%:0.0160241\n",
      "Cumulative loss on 55.1%:0.0160461\n",
      "Cumulative loss on 55.6%:0.0160523\n",
      "Cumulative loss on 56.0%:0.0160269\n",
      "Cumulative loss on 56.5%:0.0160252\n",
      "Cumulative loss on 56.9%:0.0160337\n",
      "Cumulative loss on 57.4%:0.0160308\n",
      "Cumulative loss on 57.8%:0.0160359\n",
      "Cumulative loss on 58.3%:0.0160184\n",
      "Cumulative loss on 58.8%:0.0160111\n",
      "Cumulative loss on 59.2%:0.0160487\n",
      "Cumulative loss on 59.7%:0.0160365\n",
      "Cumulative loss on 60.1%:0.0160269\n",
      "Cumulative loss on 60.6%:0.0160325\n",
      "Cumulative loss on 61.1%:0.0160580\n",
      "Cumulative loss on 61.5%:0.0160705\n",
      "Cumulative loss on 62.0%:0.0160775\n",
      "Cumulative loss on 62.4%:0.0160109\n",
      "Cumulative loss on 62.9%:0.0160469\n",
      "Cumulative loss on 63.4%:0.0160426\n",
      "Cumulative loss on 63.8%:0.0160344\n",
      "Cumulative loss on 64.3%:0.0160300\n",
      "Cumulative loss on 64.7%:0.0160440\n",
      "Cumulative loss on 65.2%:0.0160468\n",
      "Cumulative loss on 65.7%:0.0160318\n",
      "Cumulative loss on 66.1%:0.0160214\n",
      "Cumulative loss on 66.6%:0.0160408\n",
      "Cumulative loss on 67.0%:0.0160094\n",
      "Cumulative loss on 67.5%:0.0160222\n",
      "Cumulative loss on 67.9%:0.0160029\n",
      "Cumulative loss on 68.4%:0.0160264\n",
      "Cumulative loss on 68.9%:0.0160118\n",
      "Cumulative loss on 69.3%:0.0159939\n",
      "Cumulative loss on 69.8%:0.0160298\n",
      "Cumulative loss on 70.2%:0.0160292\n",
      "Cumulative loss on 70.7%:0.0160129\n",
      "Cumulative loss on 71.2%:0.0160272\n",
      "Cumulative loss on 71.6%:0.0160309\n",
      "Cumulative loss on 72.1%:0.0160320\n",
      "Cumulative loss on 72.5%:0.0160047\n",
      "Cumulative loss on 73.0%:0.0160078\n",
      "Cumulative loss on 73.5%:0.0160123\n",
      "Cumulative loss on 73.9%:0.0160309\n",
      "Cumulative loss on 74.4%:0.0160102\n",
      "Cumulative loss on 74.8%:0.0160190\n",
      "Cumulative loss on 75.3%:0.0160306\n",
      "Cumulative loss on 75.8%:0.0160237\n",
      "Cumulative loss on 76.2%:0.0160181\n",
      "Cumulative loss on 76.7%:0.0160118\n",
      "Cumulative loss on 77.1%:0.0160124\n",
      "Cumulative loss on 77.6%:0.0160037\n",
      "Cumulative loss on 78.0%:0.0160211\n",
      "Cumulative loss on 78.5%:0.0160210\n",
      "Cumulative loss on 79.0%:0.0159944\n",
      "Cumulative loss on 79.4%:0.0160206\n",
      "Cumulative loss on 79.9%:0.0159933\n",
      "Cumulative loss on 80.3%:0.0160078\n",
      "Cumulative loss on 80.8%:0.0160161\n",
      "Cumulative loss on 81.3%:0.0160029\n",
      "Cumulative loss on 81.7%:0.0160259\n",
      "Cumulative loss on 82.2%:0.0160062\n",
      "Cumulative loss on 82.6%:0.0160080\n",
      "Cumulative loss on 83.1%:0.0160209\n",
      "Cumulative loss on 83.6%:0.0160068\n",
      "Cumulative loss on 84.0%:0.0159908\n",
      "Cumulative loss on 84.5%:0.0160383\n",
      "Cumulative loss on 84.9%:0.0160134\n",
      "Cumulative loss on 85.4%:0.0160195\n",
      "Cumulative loss on 85.9%:0.0160026\n",
      "Cumulative loss on 86.3%:0.0159989\n",
      "Cumulative loss on 86.8%:0.0159968\n",
      "Cumulative loss on 87.2%:0.0159712\n",
      "Cumulative loss on 87.7%:0.0160167\n",
      "Cumulative loss on 88.1%:0.0159810\n",
      "Cumulative loss on 88.6%:0.0159950\n",
      "Cumulative loss on 89.1%:0.0159761\n",
      "Cumulative loss on 89.5%:0.0160035\n",
      "Cumulative loss on 90.0%:0.0159894\n",
      "Cumulative loss on 90.4%:0.0160128\n",
      "Cumulative loss on 90.9%:0.0160124\n",
      "Cumulative loss on 91.4%:0.0159847\n",
      "Cumulative loss on 91.8%:0.0159897\n",
      "Cumulative loss on 92.3%:0.0160234\n",
      "Cumulative loss on 92.7%:0.0159963\n",
      "Cumulative loss on 93.2%:0.0159842\n",
      "Cumulative loss on 93.7%:0.0159810\n",
      "Cumulative loss on 94.1%:0.0160023\n",
      "Cumulative loss on 94.6%:0.0160035\n",
      "Cumulative loss on 95.0%:0.0160199\n",
      "Cumulative loss on 95.5%:0.0159825\n",
      "Cumulative loss on 96.0%:0.0159914\n",
      "Cumulative loss on 96.4%:0.0160011\n",
      "Cumulative loss on 96.9%:0.0159937\n",
      "Cumulative loss on 97.3%:0.0160151\n",
      "Cumulative loss on 97.8%:0.0160035\n",
      "Cumulative loss on 98.2%:0.0159690\n",
      "Cumulative loss on 98.7%:0.0159573\n",
      "Cumulative loss on 99.2%:0.0159935\n",
      "Cumulative loss on 99.6%:0.0160127\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/batcher/batcher(vocab-20000)(batch-5000)(wind-2)(learning_finished-14-42 04-03-2019).pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-a950f5ab7fb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     with open(f'./data/batcher/batcher(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n\u001b[1;32m     46\u001b[0m            \u001b[0;34mf'(wind-{WINDOW_SIZE})'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m            f'(learning_finished-{learning_ended}).pickle', 'wb') as f:\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/batcher/batcher(vocab-20000)(batch-5000)(wind-2)(learning_finished-14-42 04-03-2019).pickle'"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    learning_started = datetime.datetime.now()\n",
    "\n",
    "    cumulative_loss = 0\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for i, (context, target) in enumerate(batcher):\n",
    "            tensor_context = torch.from_numpy(context).type(torch.cuda.LongTensor)\n",
    "            tensor_target = torch.from_numpy(target).type(torch.cuda.LongTensor)\n",
    "            tensor_context.to(device)\n",
    "            tensor_target.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            log_probs = model(tensor_context)\n",
    "            loss = loss_fun(log_probs, tensor_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss\n",
    "\n",
    "            if i % LOGS_PERIOD == 0:\n",
    "                print(f'Cumulative loss on {(i * BATCH_SIZE / corpus_size) * 100:.1f}%:' + \\\n",
    "                      f'{(cumulative_loss / BATCH_SIZE) :.7f}')\n",
    "                loss_history.append(loss.data)\n",
    "                cumulative_loss = 0\n",
    "        \n",
    "        \n",
    "        # after every epoch we save:\n",
    "                                    # the model\n",
    "                                    # loss history\n",
    "        learning_ended = datetime.datetime.now()\n",
    "        learning_time = (learning_ended - learning_started).total_seconds()\n",
    "        learning_ended = learning_ended.strftime(\"%H-%M %d-%m-%Y\")\n",
    "\n",
    "        torch.save(model, f'./models/skipgram(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                   f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                   f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                   f'(finished-{learning_ended}).pytorchmodel')\n",
    "\n",
    "        with open(f'./data/loss/loss_history(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                  f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                  f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                  f'(finished-{learning_ended}).pickle', 'wb') as f:\n",
    "            pickle.dump(loss_history, f)\n",
    "\n",
    "    with open(f'./data/batcher/batcher(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "           f'(wind-{WINDOW_SIZE})'+ \\\n",
    "           f'(learning_finished-{learning_ended}).pickle', 'wb') as f:\n",
    "        pickle.dump(batcher, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learning_ended = datetime.datetime.now()\n",
    "    learning_time = (learning_ended - learning_started).total_seconds()\n",
    "    learning_ended = learning_ended.strftime(\"%H-%M %d-%m-%Y\")\n",
    "    \n",
    "    torch.save(model, f'./models/skipgram(epoch_num-{EPOCH_NUM})(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "               f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "               f'(finished-{learning_ended}).pytorchmodel')\n",
    "    \n",
    "    with open(f'./data/loss/loss_history(epoch_num-{EPOCH_NUM})(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "           f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "           f'(finished-{learning_ended}).pickle', 'wb') as f:\n",
    "        pickle.dump(loss_history, f)\n",
    "        \n",
    "    with open(f'./data/batcher/batcher(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "           f'(wind-{WINDOW_SIZE})'+ \\\n",
    "           f'(learning_finished-{learning_ended}).pickle', 'wb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 105 minutes to train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_moving_average\n",
    "\n",
    "with open(f'./data/loss/loss_history(epoch_num-2)(vocab-5000)(batch-50)' + \\\n",
    "          '(emb-100)(wind-2)(consumed-3083.799288)(finished-16-33 03-03-2019).pickle', 'rb') as f:\n",
    "    loss_history = pickle.load(f)\n",
    "# transform from 1x1 tensor to np array\n",
    "loss_history = np.asarray([entry.data.numpy().item() for entry in loss_history], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(pd.Series(loss_history), 128, plot_actual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to map token (and word) to corresponding word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EmbeddingsEval\n",
    "trained_model = torch.load('./models/skipgram(epoch_num-2)(vocab-5000)(batch-50)' + \\\n",
    "                           '(emb-100)(wind-2)(consumed-3083.799288)(finished-16-33 03-03-2019).pytorchmodel')\n",
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intristic_matrix =  (trained_model.embedding_layer.weight.data.numpy() +\n",
    "                     trained_model.linear_layer.weight.data.numpy()) / 2\n",
    "\n",
    "\n",
    "emb_eval = EmbeddingsEval(intristic_matrix, words_to_tokens=batcher.words_to_tokens,\n",
    "                          tokens_to_words=batcher.tokens_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eval.tokens_to_embeddings([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beautiful visualizations (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 200\n",
    "words = batcher.tokens_to_words(np.arange(0, num_words))\n",
    "embeddings = emb_eval.tokens_to_embeddings(np.arange(0, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "points2d = pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.scatter(points2d[:, 0], points2d[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (points2d[i, 0] + 0.01, points2d[i, 1] + 0.01), fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qualitative evaluations of word vectors: nearest neighbors, word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_list in emb_eval.tokens_to_neighbours(batcher.words_to_tokens(['paris', 'france', 'king'])):\n",
    "    print(batcher.tokens_to_words(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = emb_eval.most_similar(positive=['old', 'buy'], negative=['sell'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
