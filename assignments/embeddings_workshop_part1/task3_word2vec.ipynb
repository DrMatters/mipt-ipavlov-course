{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skipgram import SkipGram, SkipGramBatcher\n",
    "import torch\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select whether to train model during this run or just load it from saved file\n",
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 5000\n",
    "EMBEDDINGS_DIM = 150\n",
    "EPOCH_NUM = 2\n",
    "WINDOW_SIZE = 2\n",
    "LOGS_PERIOD = 10\n",
    "MODEL_NAME = 'naive_sg'\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus into batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "with open('./data/text8', 'r') as text8:\n",
    "    text = text8.read().split()\n",
    "\n",
    "# text = ['first', 'used', 'against', 'early', 'working', 'radicals', 'including', 'class', 'other']\n",
    "batcher = SkipGramBatcher(corpus=text, vocab_size=VOCAB_SIZE,\n",
    "                          batch_size=BATCH_SIZE, window_size=WINDOW_SIZE,\n",
    "                          drop_stop_words=True)\n",
    "# free memory\n",
    "text = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    loss_history = []\n",
    "    corpus_size = len(batcher._corpus_indexes)\n",
    "\n",
    "    model = SkipGram(VOCAB_SIZE, EMBEDDINGS_DIM)\n",
    "    model.to(device)\n",
    "    loss_fun = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    learning_started = datetime.datetime.now()\n",
    "\n",
    "    cumulative_loss = 0\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for i, (context, target) in enumerate(batcher):\n",
    "            tensor_context = torch.from_numpy(context).type(torch.LongTensor)\n",
    "            tensor_target = torch.from_numpy(target).type(torch.LongTensor)\n",
    "            tensor_context.to(device)\n",
    "            tensor_target.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            log_probs = model(tensor_context)\n",
    "            print(log_probs.shape)\n",
    "            loss = loss_fun(log_probs, tensor_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss\n",
    "\n",
    "            if i % LOGS_PERIOD == 0:\n",
    "                print(f'Cumulative loss on {(i * BATCH_SIZE / corpus_size) * 100:.1f}%:' + \\\n",
    "                      f'{(cumulative_loss / LOGS_PERIOD) :.7f}')\n",
    "                loss_history.append(loss.data.cpu().item())\n",
    "                cumulative_loss = 0\n",
    "        \n",
    "        \n",
    "        # after every epoch we save:\n",
    "                                    # the model\n",
    "                                    # loss history\n",
    "        learning_ended = datetime.datetime.now()\n",
    "        learning_time = (learning_ended - learning_started).total_seconds()\n",
    "        learning_ended = learning_ended.strftime(\"%H-%M %d-%m-%Y\")\n",
    "\n",
    "        torch.save(model, f'./models/{MODEL_NAME}(finished-{learning_ended})(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                   f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                   f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                   f'.pytorchmodel')\n",
    "\n",
    "        with open(f'./data/loss/loss_{MODEL_NAME}(finished-{learning_ended})(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                  f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                  f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                  f'.pickle', 'wb') as f:\n",
    "            pickle.dump(loss_history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    with open(f'./data/batcher/batcher(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "           f'(wind-{WINDOW_SIZE})'+ \\\n",
    "           f'(learning_finished-{learning_ended}).pickle', 'wb') as f:\n",
    "        pickle.dump(batcher, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It took 105 minutes to train model on cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_moving_average\n",
    "\n",
    "with open(f'./data/loss/loss_history(epochs_completed-1)(epoch_num-2)(vocab-20000)' + \\\n",
    "          '(batch-5000)(emb-150)(wind-2)(consumed-629.121845)(finished-14-42 04-03-2019).pickle', 'rb') as f:\n",
    "    loss_history = pickle.load(f)\n",
    "# transform from 1x1 tensor to np array\n",
    "# loss_history = np.asarray([entry.data.numpy().item() for entry in loss_history], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(pd.Series(loss_history), 32, plot_actual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to map token (and word) to corresponding word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EmbeddingsEval\n",
    "trained_model = torch.load('./models/skipgram(epochs_completed-1)(epoch_num-2)(vocab-20000)(batch-5000)' + \\\n",
    "                           '(emb-150)(wind-2)(consumed-629.121845)(finished-14-42 04-03-2019).pytorchmodel', map_location='cpu')\n",
    "trained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intristic_matrix =  (trained_model.embedding_layer.weight.data.numpy() +\n",
    "                     trained_model.linear_layer.weight.data.numpy()) / 2\n",
    "\n",
    "\n",
    "emb_eval = EmbeddingsEval(intristic_matrix, words_to_tokens=batcher.words_to_tokens,\n",
    "                          tokens_to_words=batcher.tokens_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eval.tokens_to_embeddings([1, 2, 3])\n",
    "emb_eval.words_to_embeddings(['integrity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beautiful visualizations (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 200\n",
    "words = batcher.tokens_to_words(np.arange(0, num_words))\n",
    "embeddings = emb_eval.tokens_to_embeddings(np.arange(0, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "points2d = pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.scatter(points2d[:, 0], points2d[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (points2d[i, 0] + 0.01, points2d[i, 1] + 0.01), fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qualitative evaluations of word vectors: nearest neighbors, word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_list in emb_eval.tokens_to_neighbours(batcher.words_to_tokens(['paris', 'france', 'peace'])):\n",
    "    print(batcher.tokens_to_words(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = emb_eval.most_similar(positive=['mother', 'man'], negative=['woman'])\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see the expected word \"father\" is present here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
