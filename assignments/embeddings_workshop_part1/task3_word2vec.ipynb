{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skipgram import SkipGram, SkipGramBatcher\n",
    "import torch\n",
    "import gc\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 50\n",
    "EMBEDDINGS_DIM = 100\n",
    "EPOCH_NUM = 2\n",
    "WINDOW_SIZE = 2\n",
    "LOGS_PERIOD = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus into batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "with open('./data/text8', 'r') as text8:\n",
    "    text = text8.read().split()\n",
    "\n",
    "# text = ['first', 'used', 'against', 'early', 'working', 'radicals', 'including', 'class', 'other']\n",
    "batcher = SkipGramBatcher(corpus=text, vocab_size=VOCAB_SIZE, batch_size=BATCH_SIZE, window_size=WINDOW_SIZE)\n",
    "\n",
    "# free memory\n",
    "text = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "corpus_size = len(batcher.corpus_indexes)\n",
    "\n",
    "model = SkipGram(VOCAB_SIZE, EMBEDDINGS_DIM)\n",
    "loss_fun = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_started = datetime.datetime.now()\n",
    "\n",
    "cumulative_loss = 0\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    for i, (context, target) in enumerate(batcher):\n",
    "        tensor_context = torch.from_numpy(context).type(torch.LongTensor)\n",
    "        tensor_target = torch.from_numpy(target).type(torch.LongTensor)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        log_probs = model(tensor_context)\n",
    "        loss = loss_fun(log_probs, tensor_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulative_loss += loss\n",
    "\n",
    "        if i % LOGS_PERIOD == 0:\n",
    "            print(f'Cumulative loss on {(i * BATCH_SIZE / corpus_size) * 100:.1f}%: {cumulative_loss:.3f}')\n",
    "            loss_history.append(loss.data)\n",
    "            cumulative_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_ended = datetime.datetime.now()\n",
    "learning_time = (learning_ended - learning_started).total_seconds()\n",
    "learning_ended = learning_ended.strftime(\"%H-%M %d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'./models/skipgram(epoch_num-{EPOCH_NUM})(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "           f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "           f'(finished-{learning_ended}).pytorchmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/loss/loss_history(epoch_num-{EPOCH_NUM})(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "           f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "           f'(finished-{learning_ended}).pickle', 'wb') as f:\n",
    "    pickle.dump(loss_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
