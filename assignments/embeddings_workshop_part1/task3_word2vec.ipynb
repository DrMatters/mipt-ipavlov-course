{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import gc\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SkipGramBatcher():\n",
    "    def __init__(self, corpus, window_size=2, batch_size=3, vocab_size=5000, unk_text='<UNK>'):\n",
    "        self.window_size = window_size\n",
    "        self.vocab_size = vocab_size - 1\n",
    "        self.batch_size = batch_size\n",
    "        self.unk_text = unk_text\n",
    "\n",
    "        # 1. Count all word occurencies.\n",
    "        counted_words = Counter(corpus).most_common(self.vocab_size)\n",
    "        # create dict using dict comprehension\n",
    "        self.idx_to_word = {idx: word for idx, (word, count) in enumerate(counted_words)}\n",
    "        self.word_to_idx = {word: idx for idx, (word, count) in enumerate(counted_words)}\n",
    "\n",
    "        # append '<UNK>' token to dictionaries\n",
    "        last_idx = len(self.idx_to_word)\n",
    "        self.idx_to_word[last_idx] = self.unk_text\n",
    "        self.word_to_idx[self.unk_text] = last_idx\n",
    "        indexed = self.words_to_indexes(corpus)\n",
    "        \n",
    "        # transform corpus from strings to indexes, to reduce memory usage\n",
    "        self.corpus_indexes = np.asarray(\n",
    "            indexed,\n",
    "            dtype=np.int32\n",
    "        )\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    def words_to_indexes(self, words):\n",
    "        unk_index = self.word_to_idx[self.unk_text]\n",
    "        idxes = [self.word_to_idx.get(word, unk_index) for word in words]\n",
    "        return idxes\n",
    "\n",
    "    def indexes_to_words(self, indexes):\n",
    "        words = [self.idx_to_word[index] for index in indexes]\n",
    "        return words\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.batch_start_pos = 0\n",
    "        return self\n",
    "\n",
    "    def get_random_sample(self, center_id):\n",
    "        left_window = np.arange(max(0, center_id - self.window_size),\n",
    "                                center_id)\n",
    "        right_window = np.arange(center_id + 1,\n",
    "                                 min(center_id + self.window_size + 1, len(self.corpus_indexes)))\n",
    "        window = np.concatenate((left_window, right_window))\n",
    "        position = np.random.choice(window)\n",
    "        return self.corpus_indexes[position]\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.batch_start_pos >= len(self.corpus_indexes):\n",
    "            raise StopIteration\n",
    "        else:\n",
    "            batch_position_in_corpus = np.arange(\n",
    "                self.batch_start_pos,\n",
    "                min(self.batch_start_pos + self.batch_size, len(self.corpus_indexes))\n",
    "            )\n",
    "            x_batch = np.asarray(self.corpus_indexes[batch_position_in_corpus])\n",
    "            # draw a word from window of a selected word\n",
    "            y_batch = np.asarray([self.get_random_sample(selected_word_position)\n",
    "                                  for selected_word_position in batch_position_in_corpus]).flatten()\n",
    "            # for selected_word_position in batch_position_in_corpus:\n",
    "            #     y_batch.append(self.get_random_sample(selected_word_position))\n",
    "            self.batch_start_pos += self.batch_size\n",
    "            return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 500\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus into batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = []\n",
    "with open('./data/text8', 'r') as text8:\n",
    "    text = text8.read().split()\n",
    "text = text[:1000]\n",
    "\n",
    "# text = ['first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'class', 'other']\n",
    "batcher = SkipGramBatcher(corpus=text, vocab_size=VOCAB_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "# free memory\n",
    "text = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at batch 0: 10.238473892211914\n",
      "Loss at batch 10: 11.569684982299805\n",
      "Loss at batch 20: 8.318129539489746\n",
      "Loss at batch 30: 13.311193466186523\n",
      "Loss at batch 40: 11.74460506439209\n",
      "Loss at batch 50: 6.547219753265381\n",
      "Loss at batch 60: 10.56187629699707\n",
      "Loss at batch 70: 11.087947845458984\n",
      "Loss at batch 80: 10.150960922241211\n",
      "Loss at batch 90: 7.417031764984131\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = 50\n",
    "W1 = Variable(torch.randn(VOCAB_SIZE, embedding_dims).float(), requires_grad=True)\n",
    "W2 = Variable(torch.randn(embedding_dims, VOCAB_SIZE).float(), requires_grad=True)\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# tensor to store oh-encoded data\n",
    "x_onehot = torch.FloatTensor(BATCH_SIZE, VOCAB_SIZE)\n",
    "\n",
    "quanity = 10\n",
    "loss_history = []\n",
    "\n",
    "for epo in range(num_epochs):\n",
    "    for i, (data, target) in enumerate(batcher):\n",
    "        # if we have not full batch (at the end of the corpus), OH-encoding will not work\n",
    "        if data.shape[0] != BATCH_SIZE:\n",
    "            continue\n",
    "        \n",
    "        # get data (center word) and target (word, sampled from window)\n",
    "        tensor_data = torch.from_numpy(np.expand_dims(data, axis=1)).type(torch.LongTensor)\n",
    "        tensor_target = torch.from_numpy(target).type(torch.LongTensor)\n",
    "        \n",
    "        # get input and expected data\n",
    "        x_onehot.zero_()\n",
    "        x_onehot.scatter_(1, tensor_data, 1)\n",
    "        y = tensor_target\n",
    "\n",
    "        # forward propagate\n",
    "        z1 = torch.matmul(x_onehot, W1)\n",
    "        z2 = torch.matmul(z1, W2)\n",
    "        \n",
    "        # apply log softmax\n",
    "        log_softmax = torch.nn.functional.log_softmax(z2, dim=0)\n",
    "        # apply the negative log likelihood loss\n",
    "        # nll_loss takes (batch_size, vocab_size)\n",
    "        loss = torch.nn.functional.nll_loss(log_softmax, y)\n",
    "        \n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        \n",
    "        # learning step\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "        \n",
    "        if (i % quanity) == 0:\n",
    "            print(f'Loss at batch {i}: {loss}')\n",
    "#             print(f'update step:\\n {W1.grad.data}')\n",
    "            loss_history.append(loss)\n",
    "        \n",
    "        # delete backpropagation data\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
