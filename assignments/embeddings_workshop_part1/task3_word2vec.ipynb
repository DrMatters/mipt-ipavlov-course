{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive word2vec\n",
    "\n",
    "This task can be formulated very simply. Follow this [paper](https://arxiv.org/pdf/1411.2738.pdf) and implement word2vec like a two-layer neural network with matrices $W$ and $W'$. One matrix projects words to low-dimensional 'hidden' space and the other - back to high-dimensional vocabulary space.\n",
    "\n",
    "![word2vec](https://i.stack.imgur.com/6eVXZ.jpg)\n",
    "\n",
    "You can use TensorFlow/PyTorch (numpy too, if you love to calculate gradients on your own and want some extra points, but don't forget to numerically check your gradients) and code from your previous task. Again: you don't have to implement negative sampling (you may reduce your vocabulary size for faster computation).\n",
    "\n",
    "**Results of this task**:\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "Also, you can find any other datasets for quantitative evaluation. If you chose to do this, please use the same datasets across tasks 3, 4, 5 and 6.\n",
    "\n",
    "Again. It is **highly recommended** to read this [paper](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "\n",
    "Example of visualization in tensorboard:\n",
    "https://projector.tensorflow.org\n",
    "\n",
    "Example of 2D visualisation:\n",
    "\n",
    "![2dword2vec](https://www.tensorflow.org/images/tsne.png)\n",
    "\n",
    "If you struggle with something, ask your neighbor. If it is not obvious for you, probably someone else is looking for the answer too. And in contrast, if you see that you can help someone - do it! Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skipgram import SkipGram, SkipGramBatcher\n",
    "import torch\n",
    "import gc\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Constant to control how frequently we print train loss\n",
    "print_every = 100\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 50\n",
    "EMBEDDINGS_DIM = 100\n",
    "EPOCH_NUM = 1\n",
    "WINDOW_SIZE = 2\n",
    "LOGS_PERIOD = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus into batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = []\n",
    "with open('./data/text8', 'r') as text8:\n",
    "    text = text8.read().split()\n",
    "\n",
    "# text = ['first', 'used', 'against', 'early', 'working', 'radicals', 'including', 'class', 'other']\n",
    "batcher = SkipGramBatcher(corpus=text, vocab_size=VOCAB_SIZE, batch_size=BATCH_SIZE, window_size=WINDOW_SIZE)\n",
    "\n",
    "# free memory\n",
    "text = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "corpus_size = len(batcher.corpus_indexes)\n",
    "\n",
    "model = SkipGram(VOCAB_SIZE, EMBEDDINGS_DIM)\n",
    "loss_fun = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative loss on 0.0%: 3.993\n",
      "Cumulative loss on 0.1%: 2015.029\n",
      "Cumulative loss on 0.3%: 2001.717\n",
      "Cumulative loss on 0.4%: 1987.739\n",
      "Cumulative loss on 0.6%: 1985.379\n",
      "Cumulative loss on 0.7%: 1983.088\n",
      "Cumulative loss on 0.9%: 1978.782\n",
      "Cumulative loss on 1.0%: 1974.230\n",
      "Cumulative loss on 1.2%: 1976.489\n",
      "Cumulative loss on 1.3%: 1966.361\n",
      "Cumulative loss on 1.5%: 1965.688\n",
      "Cumulative loss on 1.6%: 1968.684\n",
      "Cumulative loss on 1.8%: 1970.624\n",
      "Cumulative loss on 1.9%: 1969.521\n",
      "Cumulative loss on 2.1%: 1968.504\n",
      "Cumulative loss on 2.2%: 1963.109\n",
      "Cumulative loss on 2.4%: 1959.245\n",
      "Cumulative loss on 2.5%: 1960.104\n",
      "Cumulative loss on 2.6%: 1958.563\n",
      "Cumulative loss on 2.8%: 1958.896\n",
      "Cumulative loss on 2.9%: 1961.895\n",
      "Cumulative loss on 3.1%: 1953.100\n",
      "Cumulative loss on 3.2%: 1941.346\n",
      "Cumulative loss on 3.4%: 1956.129\n",
      "Cumulative loss on 3.5%: 1950.164\n",
      "Cumulative loss on 3.7%: 1953.703\n",
      "Cumulative loss on 3.8%: 1949.626\n",
      "Cumulative loss on 4.0%: 1960.914\n",
      "Cumulative loss on 4.1%: 1959.816\n",
      "Cumulative loss on 4.3%: 1951.541\n",
      "Cumulative loss on 4.4%: 1945.882\n",
      "Cumulative loss on 4.6%: 1947.094\n",
      "Cumulative loss on 4.7%: 1954.825\n",
      "Cumulative loss on 4.9%: 1944.614\n",
      "Cumulative loss on 5.0%: 1951.761\n",
      "Cumulative loss on 5.1%: 1948.606\n",
      "Cumulative loss on 5.3%: 1946.035\n",
      "Cumulative loss on 5.4%: 1951.622\n",
      "Cumulative loss on 5.6%: 1925.195\n",
      "Cumulative loss on 5.7%: 1941.249\n",
      "Cumulative loss on 5.9%: 1942.191\n",
      "Cumulative loss on 6.0%: 1935.054\n",
      "Cumulative loss on 6.2%: 1932.366\n",
      "Cumulative loss on 6.3%: 1933.789\n",
      "Cumulative loss on 6.5%: 1937.736\n",
      "Cumulative loss on 6.6%: 1938.016\n",
      "Cumulative loss on 6.8%: 1938.855\n",
      "Cumulative loss on 6.9%: 1942.328\n",
      "Cumulative loss on 7.1%: 1944.819\n",
      "Cumulative loss on 7.2%: 1942.941\n",
      "Cumulative loss on 7.4%: 1944.894\n",
      "Cumulative loss on 7.5%: 1943.638\n",
      "Cumulative loss on 7.6%: 1957.244\n",
      "Cumulative loss on 7.8%: 1940.499\n",
      "Cumulative loss on 7.9%: 1951.280\n",
      "Cumulative loss on 8.1%: 1947.913\n",
      "Cumulative loss on 8.2%: 1948.723\n",
      "Cumulative loss on 8.4%: 1944.400\n",
      "Cumulative loss on 8.5%: 1949.904\n",
      "Cumulative loss on 8.7%: 1939.830\n",
      "Cumulative loss on 8.8%: 1945.692\n",
      "Cumulative loss on 9.0%: 1935.791\n",
      "Cumulative loss on 9.1%: 1950.915\n",
      "Cumulative loss on 9.3%: 1940.454\n",
      "Cumulative loss on 9.4%: 1940.963\n",
      "Cumulative loss on 9.6%: 1944.102\n",
      "Cumulative loss on 9.7%: 1937.505\n",
      "Cumulative loss on 9.8%: 1943.145\n",
      "Cumulative loss on 10.0%: 1942.928\n",
      "Cumulative loss on 10.1%: 1940.848\n",
      "Cumulative loss on 10.3%: 1943.191\n",
      "Cumulative loss on 10.4%: 1942.355\n",
      "Cumulative loss on 10.6%: 1938.971\n",
      "Cumulative loss on 10.7%: 1935.276\n",
      "Cumulative loss on 10.9%: 1939.866\n",
      "Cumulative loss on 11.0%: 1933.559\n",
      "Cumulative loss on 11.2%: 1933.758\n",
      "Cumulative loss on 11.3%: 1942.773\n",
      "Cumulative loss on 11.5%: 1945.373\n",
      "Cumulative loss on 11.6%: 1945.129\n",
      "Cumulative loss on 11.8%: 1942.359\n",
      "Cumulative loss on 11.9%: 1946.483\n",
      "Cumulative loss on 12.1%: 1943.342\n",
      "Cumulative loss on 12.2%: 1942.606\n",
      "Cumulative loss on 12.3%: 1938.328\n",
      "Cumulative loss on 12.5%: 1945.310\n",
      "Cumulative loss on 12.6%: 1944.703\n",
      "Cumulative loss on 12.8%: 1945.412\n",
      "Cumulative loss on 12.9%: 1934.705\n",
      "Cumulative loss on 13.1%: 1941.401\n",
      "Cumulative loss on 13.2%: 1939.654\n",
      "Cumulative loss on 13.4%: 1938.568\n",
      "Cumulative loss on 13.5%: 1940.260\n",
      "Cumulative loss on 13.7%: 1939.036\n",
      "Cumulative loss on 13.8%: 1945.180\n",
      "Cumulative loss on 14.0%: 1945.292\n",
      "Cumulative loss on 14.1%: 1937.337\n",
      "Cumulative loss on 14.3%: 1928.472\n",
      "Cumulative loss on 14.4%: 1939.409\n",
      "Cumulative loss on 14.6%: 1939.125\n",
      "Cumulative loss on 14.7%: 1934.094\n",
      "Cumulative loss on 14.8%: 1937.305\n",
      "Cumulative loss on 15.0%: 1943.388\n",
      "Cumulative loss on 15.1%: 1935.094\n",
      "Cumulative loss on 15.3%: 1942.758\n",
      "Cumulative loss on 15.4%: 1942.000\n",
      "Cumulative loss on 15.6%: 1936.170\n",
      "Cumulative loss on 15.7%: 1940.775\n",
      "Cumulative loss on 15.9%: 1944.094\n",
      "Cumulative loss on 16.0%: 1938.098\n",
      "Cumulative loss on 16.2%: 1937.037\n",
      "Cumulative loss on 16.3%: 1937.032\n",
      "Cumulative loss on 16.5%: 1936.040\n",
      "Cumulative loss on 16.6%: 1925.904\n",
      "Cumulative loss on 16.8%: 1932.699\n",
      "Cumulative loss on 16.9%: 1940.585\n",
      "Cumulative loss on 17.1%: 1939.687\n",
      "Cumulative loss on 17.2%: 1943.070\n",
      "Cumulative loss on 17.3%: 1933.292\n",
      "Cumulative loss on 17.5%: 1935.927\n",
      "Cumulative loss on 17.6%: 1942.397\n",
      "Cumulative loss on 17.8%: 1934.264\n",
      "Cumulative loss on 17.9%: 1940.820\n",
      "Cumulative loss on 18.1%: 1935.928\n",
      "Cumulative loss on 18.2%: 1933.233\n",
      "Cumulative loss on 18.4%: 1944.445\n",
      "Cumulative loss on 18.5%: 1932.009\n",
      "Cumulative loss on 18.7%: 1938.961\n",
      "Cumulative loss on 18.8%: 1936.737\n",
      "Cumulative loss on 19.0%: 1927.231\n",
      "Cumulative loss on 19.1%: 1939.525\n",
      "Cumulative loss on 19.3%: 1941.332\n",
      "Cumulative loss on 19.4%: 1928.458\n",
      "Cumulative loss on 19.6%: 1937.959\n",
      "Cumulative loss on 19.7%: 1932.625\n",
      "Cumulative loss on 19.8%: 1931.164\n",
      "Cumulative loss on 20.0%: 1941.174\n",
      "Cumulative loss on 20.1%: 1933.145\n",
      "Cumulative loss on 20.3%: 1932.407\n",
      "Cumulative loss on 20.4%: 1929.605\n",
      "Cumulative loss on 20.6%: 1941.740\n",
      "Cumulative loss on 20.7%: 1936.551\n",
      "Cumulative loss on 20.9%: 1929.488\n",
      "Cumulative loss on 21.0%: 1938.326\n",
      "Cumulative loss on 21.2%: 1933.241\n",
      "Cumulative loss on 21.3%: 1931.767\n",
      "Cumulative loss on 21.5%: 1935.440\n",
      "Cumulative loss on 21.6%: 1933.234\n",
      "Cumulative loss on 21.8%: 1928.091\n",
      "Cumulative loss on 21.9%: 1933.835\n",
      "Cumulative loss on 22.1%: 1930.906\n",
      "Cumulative loss on 22.2%: 1929.547\n",
      "Cumulative loss on 22.3%: 1929.373\n",
      "Cumulative loss on 22.5%: 1939.226\n",
      "Cumulative loss on 22.6%: 1941.318\n",
      "Cumulative loss on 22.8%: 1937.630\n",
      "Cumulative loss on 22.9%: 1934.591\n",
      "Cumulative loss on 23.1%: 1940.607\n",
      "Cumulative loss on 23.2%: 1946.054\n",
      "Cumulative loss on 23.4%: 1935.158\n",
      "Cumulative loss on 23.5%: 1938.455\n",
      "Cumulative loss on 23.7%: 1938.194\n",
      "Cumulative loss on 23.8%: 1940.120\n",
      "Cumulative loss on 24.0%: 1941.163\n",
      "Cumulative loss on 24.1%: 1934.363\n",
      "Cumulative loss on 24.3%: 1922.332\n",
      "Cumulative loss on 24.4%: 1923.953\n",
      "Cumulative loss on 24.6%: 1933.173\n",
      "Cumulative loss on 24.7%: 1925.575\n",
      "Cumulative loss on 24.8%: 1929.099\n",
      "Cumulative loss on 25.0%: 1934.411\n",
      "Cumulative loss on 25.1%: 1931.462\n",
      "Cumulative loss on 25.3%: 1935.804\n",
      "Cumulative loss on 25.4%: 1939.228\n",
      "Cumulative loss on 25.6%: 1933.756\n",
      "Cumulative loss on 25.7%: 1934.941\n",
      "Cumulative loss on 25.9%: 1936.578\n",
      "Cumulative loss on 26.0%: 1932.832\n",
      "Cumulative loss on 26.2%: 1933.894\n",
      "Cumulative loss on 26.3%: 1940.127\n",
      "Cumulative loss on 26.5%: 1926.469\n",
      "Cumulative loss on 26.6%: 1938.655\n",
      "Cumulative loss on 26.8%: 1939.628\n",
      "Cumulative loss on 26.9%: 1936.568\n",
      "Cumulative loss on 27.1%: 1922.292\n",
      "Cumulative loss on 27.2%: 1928.904\n",
      "Cumulative loss on 27.3%: 1933.876\n",
      "Cumulative loss on 27.5%: 1933.732\n",
      "Cumulative loss on 27.6%: 1942.567\n",
      "Cumulative loss on 27.8%: 1940.412\n",
      "Cumulative loss on 27.9%: 1937.788\n",
      "Cumulative loss on 28.1%: 1928.084\n",
      "Cumulative loss on 28.2%: 1931.562\n",
      "Cumulative loss on 28.4%: 1925.228\n",
      "Cumulative loss on 28.5%: 1930.776\n",
      "Cumulative loss on 28.7%: 1935.145\n",
      "Cumulative loss on 28.8%: 1929.874\n",
      "Cumulative loss on 29.0%: 1931.978\n",
      "Cumulative loss on 29.1%: 1931.667\n",
      "Cumulative loss on 29.3%: 1944.188\n",
      "Cumulative loss on 29.4%: 1932.554\n",
      "Cumulative loss on 29.5%: 1941.259\n",
      "Cumulative loss on 29.7%: 1939.631\n",
      "Cumulative loss on 29.8%: 1932.479\n",
      "Cumulative loss on 30.0%: 1927.237\n",
      "Cumulative loss on 30.1%: 1927.653\n",
      "Cumulative loss on 30.3%: 1933.229\n",
      "Cumulative loss on 30.4%: 1935.376\n",
      "Cumulative loss on 30.6%: 1926.777\n",
      "Cumulative loss on 30.7%: 1934.244\n",
      "Cumulative loss on 30.9%: 1938.708\n",
      "Cumulative loss on 31.0%: 1936.439\n",
      "Cumulative loss on 31.2%: 1934.867\n",
      "Cumulative loss on 31.3%: 1929.709\n",
      "Cumulative loss on 31.5%: 1930.058\n",
      "Cumulative loss on 31.6%: 1923.019\n",
      "Cumulative loss on 31.8%: 1925.845\n",
      "Cumulative loss on 31.9%: 1933.215\n",
      "Cumulative loss on 32.0%: 1932.282\n",
      "Cumulative loss on 32.2%: 1933.978\n",
      "Cumulative loss on 32.3%: 1925.376\n"
     ]
    }
   ],
   "source": [
    "learning_started = datetime.datetime.now()\n",
    "\n",
    "cumulative_loss = 0\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    for i, (context, target) in enumerate(batcher):\n",
    "        tensor_context = torch.from_numpy(context).type(torch.LongTensor)\n",
    "        tensor_target = torch.from_numpy(target).type(torch.LongTensor)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        log_probs = model(tensor_context)\n",
    "        loss = loss_fun(log_probs, tensor_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cumulative_loss += loss\n",
    "\n",
    "        if i % LOGS_PERIOD == 0:\n",
    "            print(f'Cumulative loss on {(i * BATCH_SIZE / corpus_size) * 100:.1f}%: {cumulative_loss:.3f}')\n",
    "            loss_history.append(loss.data)\n",
    "            cumulative_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_ended = datetime.datetime.now()\n",
    "learning_time = (learning_ended - learning_started).total_seconds()\n",
    "learning_ended = learning_ended.strftime(\"%H:%M %d-%m-%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'./models/skipgram(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "           f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE}(consumed-{learning-time}))'+ \\ \n",
    "           f'(finished-{learning_ended}).pytorchmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
