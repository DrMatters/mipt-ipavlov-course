{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec only with one matrix\n",
    "\n",
    "Choose CBOW/Skip-Gram again. Implement it, but now without second `h_dim x vocab_size` matrix. Remember what was the loss function in Stanford lecture.\n",
    "\n",
    "$$ L = - \\frac{1}{T} \\sum_{t=1}^T \\sum_{-m <= j <= m, j \\ne 0} log P(w_{t+j}|w_{t+j-m})$$\n",
    "\n",
    "\n",
    "$$ P(w_h | w_i) = \\frac{exp(s(v_i, v_h))}{\\sum exp(s(v_i, v_w))}$$\n",
    "\n",
    "Where $s(x, y)$ is a similarity function. It is common to use dot-product here $s(x, y) = s^Ty$. The sum in the denominator is across all vocabulary for $P$ to be a probability distribution.\n",
    "\n",
    "Look to this formula closer. There is no projection back to vocabulary dimension in the loss! So, we can reduce the number of parameters by the factor of two by not using the second matrix $W'$. Your new task is to code CBOW / SkipGram with this loss.\n",
    "\n",
    "**Main idea:**\n",
    "\n",
    "We project word vectors to some space. Next, we work with the space itself. We update projection matrix weights so that similar (in the terms of Distributional semantics) words have similar vectors in that space. We don't need to solve classification task explicitly here.\n",
    "\n",
    "**Tip:**\n",
    "\n",
    "Work with matrix operations and not with nn.layers / keras.layers.\n",
    "\n",
    "**Results of this task** (the very same as in task 3):\n",
    " * trained word vectors (mention somewhere, how long it took to train)\n",
    " * plotted loss (so we can see that it has converged)\n",
    " * function to map token to corresponding word vector\n",
    " * beautiful visualizations (PCE, T-SNE), you can use TensorBoard and play with your vectors in 3D (don't forget to add screenshots to the task)\n",
    " * qualitative evaluations of word vectors: nearest neighbors, word analogies\n",
    "\n",
    "\n",
    "**Extra:**\n",
    " * quantitative evaluation:\n",
    "   * for intrinsic evaluation you can find datasets [here](https://aclweb.org/aclwiki/Analogy_(State_of_the_art))\n",
    "   * for extrincis evaluation you can use [these](https://medium.com/@dataturks/rare-text-classification-open-datasets-9d340c8c508e)\n",
    "\n",
    "\n",
    "\n",
    "Supplementary materials:\n",
    "  * [cs224n winter 2017 lecture 2 video](https://www.youtube.com/watch?v=ERibwqs9p38&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&index=2)\n",
    "  * [cs224n winter 2019 lecture 1 slides](http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture01-wordvecs1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skipgram import SkipGramBatcher, SingleMatrixSkipGram\n",
    "import torch\n",
    "import gc\n",
    "import datetime\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import EmbeddingsEval\n",
    "\n",
    "# for visualisation\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select whether to train model during this run or just load it from saved file\n",
    "TRAIN = True\n",
    "\n",
    "VOCAB_SIZE = 5000\n",
    "BATCH_SIZE = 100\n",
    "EMBEDDINGS_DIM = 50\n",
    "EPOCH_NUM = 2\n",
    "WINDOW_SIZE = 2\n",
    "LOGS_PERIOD = 1000\n",
    "MODEL_NAME = 'single_matrix'\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n",
      "using <class 'torch.LongTensor'> type\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "if device.__str__() == 'cpu':\n",
    "    tensor_type = torch.LongTensor\n",
    "else:\n",
    "    tensor_type = torch.cuda.LongTensor\n",
    "\n",
    "print('using device:', device)\n",
    "print(f'using {tensor_type} type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load corpus into batcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Alting\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = []\n",
    "# with open('./data/text8', 'r') as text8:\n",
    "#     text = text8.read().split()\n",
    "\n",
    "text = ['first', 'used', 'against', 'early', 'working', 'radicals', 'including', 'class', 'other']\n",
    "batcher = SkipGramBatcher(corpus=text, vocab_size=VOCAB_SIZE,\n",
    "                          batch_size=BATCH_SIZE, window_size=WINDOW_SIZE,\n",
    "                          drop_stop_words=True)\n",
    "# free memory\n",
    "text = []\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    loss_history = []\n",
    "    corpus_size = len(batcher._corpus_indexes)\n",
    "\n",
    "    model = SingleMatrixSkipGram(VOCAB_SIZE, EMBEDDINGS_DIM, device)\n",
    "    loss_fun = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam([model.emb_matrix], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0dac192e0888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mtensor_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0mlog_probs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Alting\\Documents\\GitHub\\mipt-ipavlov-course\\assignments\\embeddings_workshop_part1\\skipgram.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_intrinsic_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "if TRAIN:\n",
    "    learning_started = datetime.datetime.now()\n",
    "\n",
    "    cumulative_loss = 0\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        for i, (context, target) in enumerate(batcher):\n",
    "            tensor_context = torch.from_numpy(context).type(tensor_type)\n",
    "            tensor_target = torch.from_numpy(target).type(tensor_type)\n",
    "            tensor_context.to(device)\n",
    "            tensor_target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "#             model.zero_grad()\n",
    "\n",
    "            log_probs = model(tensor_context)\n",
    "            loss = loss_fun(log_probs, tensor_target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cumulative_loss += loss\n",
    "\n",
    "            if i % LOGS_PERIOD == 0:\n",
    "                print(f'loss on {(i * BATCH_SIZE / corpus_size) * 100:.1f}%: ' + \\\n",
    "                      f'{(cumulative_loss / LOGS_PERIOD) :.7f}')\n",
    "                \n",
    "                loss_history.append(loss.data.cpu().numpy().item())\n",
    "                cumulative_loss = 0\n",
    "        \n",
    "        # after every epoch we save:\n",
    "                                    # the model\n",
    "                                    # loss history\n",
    "        learning_ended = datetime.datetime.now()\n",
    "        learning_time = (learning_ended - learning_started).total_seconds()\n",
    "        learning_ended = learning_ended.strftime(\"%H-%M %d-%m-%Y\")\n",
    "\n",
    "        torch.save(model, f'./models/{MODEL_NAME}(finished-{learning_ended})(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                   f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                   f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                   f'.pytorchmodel')\n",
    "\n",
    "        with open(f'./data/loss/loss_{MODEL_NAME}(finished-{learning_ended})(epochs_completed-{epoch})(epoch_num-{EPOCH_NUM})' + \\\n",
    "                  f'(vocab-{VOCAB_SIZE})(batch-{BATCH_SIZE})' + \\\n",
    "                  f'(emb-{EMBEDDINGS_DIM})(wind-{WINDOW_SIZE})(consumed-{learning_time})'+ \\\n",
    "                  f'.pickle', 'wb') as f:\n",
    "            pickle.dump(loss_history, f)\n",
    "        print('Model saved succesfully')\n",
    "        \n",
    "        # quality evaluation\n",
    "#         intrinsic_matrix = model.get_intrinsic_matrix()\n",
    "\n",
    "#         emb_eval = EmbeddingsEval(intrinsic_matrix, words_to_tokens=batcher.words_to_tokens,\n",
    "#                                   tokens_to_words=batcher.tokens_to_words)\n",
    "        \n",
    "#         for token_list in emb_eval.words_to_neighbors(['paris', 'energy', 'water', 'number']):\n",
    "#             print(batcher.tokens_to_words(token_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_moving_average\n",
    "\n",
    "loss_file_location = ''\n",
    "\n",
    "with open(loss_file_location, 'rb') as f:\n",
    "    loss_history = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_moving_average(pd.Series(loss_history), 32, plot_actual=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to map token (and word) to corresponding word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_location = './models/skipgram(epochs_completed-1)(epoch_num-2)(vocab-20000)(batch-5000)' + \\\n",
    "    '(emb-150)(wind-2)(consumed-629.121845)(finished-14-42 04-03-2019).pytorchmodel'\n",
    "model = torch.load(model_file_location, map_location='cpu')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intrinsic_matrix = model.get_intrinsic_matrix()\n",
    "\n",
    "emb_eval = EmbeddingsEval(intrinsic_matrix, words_to_tokens=batcher.words_to_tokens,\n",
    "                          tokens_to_words=batcher.tokens_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_eval.tokens_to_embeddings([1, 2, 3])\n",
    "emb_eval.words_to_embeddings(['integrity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beautiful visualizations (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 200\n",
    "words = batcher.tokens_to_words(np.arange(0, num_words))\n",
    "embeddings = emb_eval.tokens_to_embeddings(np.arange(0, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "points2d = pca.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "ax.scatter(points2d[:, 0], points2d[:, 1])\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ax.annotate(word, (points2d[i, 0] + 0.01, points2d[i, 1] + 0.01), fontsize='small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qualitative evaluations of word vectors: nearest neighbors, word analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_list in emb_eval.words_to_neighbors(['paris', 'energy', 'water', 'number']):\n",
    "    print(batcher.tokens_to_words(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = emb_eval.most_similar(positive=['mother', 'man'], negative=['woman'])\n",
    "similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
